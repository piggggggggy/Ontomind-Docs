// =============================================================
// OntoMind â€” Step 5: LLM Port (Adapters + Validation + Streaming)
// -------------------------------------------------------------
// ëª©í‘œ: Step 4ì˜ í”„ë¡¬í”„íŠ¸ ë²ˆë“¤(system/user/toolSchema)ì„ ë°›ì•„ ì‹¤ì œ LLMì„ í˜¸ì¶œí•˜ê³ ,
//       (ê°€ëŠ¥í•˜ë©´) êµ¬ì¡°í™” ì¶œë ¥(response_format)ìœ¼ë¡œ ë°›ìœ¼ë©°, validatorë¡œ ê²°ê³¼ë¥¼ ê²€ì¦.
//       ìŠ¤íŠ¸ë¦¬ë°/ë…¼ìŠ¤íŠ¸ë¦¬ë° ëª¨ë‘ ì§€ì›.
// =============================================================

/*
ğŸ“ ì¶”ê°€ íŒŒì¼ íŠ¸ë¦¬

packages/
  port/
    src/
      index.ts
      llm.ts
      validate.ts
      streaming.ts
      providers/
        openai.ts
        openai-types.ts
    package.json

apps/
  chat/
    app/
      api/
        chat/route.ts     # Next.js(App Router) â€” ëª¨ë¸ í˜¸ì¶œ API

.env.local (ì˜ˆì‹œ)
  OPENAI_API_KEY=sk-...
  OPENAI_BASE_URL=https://api.openai.com/v1
  OPENAI_MODEL=gpt-4o-mini
*/

// =============================================================
// packages/port/src/providers/openai-types.ts â€” ìµœì†Œ íƒ€ì…
// -------------------------------------------------------------
export interface OpenAIChatMessage { role: 'system'|'user'|'assistant'|'tool'; content: string }
export interface OpenAIChatRequest {
  model: string
  messages: OpenAIChatMessage[]
  temperature?: number
  response_format?: { type: 'json_schema', json_schema: any } | { type: 'text' }
  stream?: boolean
}

export interface OpenAIChatChunkChoiceDelta { role?: string; content?: string }
export interface OpenAIChatChunkChoice { delta?: OpenAIChatChunkChoiceDelta; finish_reason?: string }
export interface OpenAIStreamChunk { choices?: OpenAIChatChunkChoice[] }

// =============================================================
// packages/port/src/providers/openai.ts â€” OpenAI ì–´ëŒ‘í„°
// -------------------------------------------------------------
import type { OpenAIChatRequest } from './openai-types'

export interface OpenAIConfig {
  apiKey: string
  baseURL?: string // ê¸°ë³¸: https://api.openai.com/v1
  model: string
}

export async function openaiChat(req: OpenAIChatRequest, cfg: OpenAIConfig) {
  const base = cfg.baseURL || process.env.OPENAI_BASE_URL || 'https://api.openai.com/v1'
  const res = await fetch(`${base}/chat/completions`, {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${cfg.apiKey}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify(req)
  })
  if (!res.ok) throw new Error(`OpenAI HTTP ${res.status}`)
  return res
}

// =============================================================
// packages/port/src/streaming.ts â€” SSE-like í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¼ íŒŒì„œ
// -------------------------------------------------------------
export async function* readServerSentEvents(res: Response) {
  const reader = res.body!.getReader()
  const decoder = new TextDecoder()
  let buf = ''
  while (true) {
    const { done, value } = await reader.read()
    if (done) break
    buf += decoder.decode(value, { stream: true })
    const chunks = buf.split('\n\n')
    buf = chunks.pop() || ''
    for (const block of chunks) {
      for (const line of block.split('\n')) {
        if (!line.startsWith('data:')) continue
        const data = line.slice(5).trim()
        if (data === '[DONE]') return
        yield data
      }
    }
  }
}

// =============================================================
// packages/port/src/validate.ts â€” ì‘ë‹µ ê²€ì¦ í—¬í¼
// -------------------------------------------------------------
import path from 'node:path'
import { validateIntentIO } from '@ontomind/validator/src/intent'

export function validateIntentOutput(intentKey: string, payload: any): { ok: boolean; errors?: string[] } {
  const intentsSchemaPath = path.join(process.cwd(), 'data', 'generated', 'intents.schema.json')
  return validateIntentIO(intentKey, payload, 'output', intentsSchemaPath)
}

// =============================================================
// packages/port/src/llm.ts â€” í”„ë¡¬í”„íŠ¸ ë²ˆë“¤ â†’ ëª¨ë¸ í˜¸ì¶œ
// -------------------------------------------------------------
import type { PromptBundle } from '@ontomind/prompt/src/types'
import { openaiChat, type OpenAIConfig } from './providers/openai'
import type { OpenAIChatRequest } from './providers/openai-types'
import { validateIntentOutput } from './validate'

export interface CallOptions {
  provider: 'openai'
  apiKey?: string
  baseURL?: string
  model?: string
  temperature?: number
  stream?: boolean
  intentKey: string
}

export async function callLLM(bundle: PromptBundle, opts: CallOptions) {
  const { system, user, toolSchema } = bundle

  if (opts.provider !== 'openai') throw new Error('Only openai provider is wired for now')
  const cfg: OpenAIConfig = {
    apiKey: opts.apiKey || process.env.OPENAI_API_KEY || '',
    baseURL: opts.baseURL || process.env.OPENAI_BASE_URL || undefined,
    model: opts.model || process.env.OPENAI_MODEL || 'gpt-4o-mini'
  }
  if (!cfg.apiKey) throw new Error('OPENAI_API_KEY missing')

  const req: OpenAIChatRequest = {
    model: cfg.model,
    messages: [
      { role: 'system', content: system },
      { role: 'user', content: user }
    ],
    temperature: opts.temperature ?? 0.2,
    response_format: toolSchema
      ? { type: 'json_schema', json_schema: { name: 'intent_output', schema: toolSchema, strict: true } }
      : { type: 'text' },
    stream: !!opts.stream
  }

  const res = await openaiChat(req, cfg)

  // ìŠ¤íŠ¸ë¦¬ë°ì´ ì•„ë‹Œ ê²½ìš°: JSON íŒŒì‹± â†’ ê²€ì¦ í›„ ë°˜í™˜
  if (!opts.stream) {
    const data = await res.json() as any
    const content = data?.choices?.[0]?.message?.content
    let parsed: any
    try { parsed = typeof content === 'string' ? JSON.parse(content) : content } catch { parsed = content }
    const check = validateIntentOutput(opts.intentKey, parsed)
    if (!check.ok) return { ok: false, error: 'schema_validation_failed', details: check.errors }
    return { ok: true, output: parsed, raw: data }
  }

  // ìŠ¤íŠ¸ë¦¬ë°ì¸ ê²½ìš°: callerì—ê²Œ Response ê·¸ëŒ€ë¡œ ë°˜í™˜(Next APIì—ì„œ ì¤‘ê³„)
  return { ok: true, stream: res.body }
}

// =============================================================
// packages/port/src/index.ts â€” Public API
// -------------------------------------------------------------
export { callLLM, type CallOptions } from './llm'
export { validateIntentOutput } from './validate'

// =============================================================
// packages/port/package.json
// -------------------------------------------------------------
{
  "name": "@ontomind/port",
  "version": "0.1.0",
  "type": "module",
  "main": "dist/index.js",
  "types": "dist/index.d.ts",
  "files": ["dist"],
  "scripts": {
    "build": "tsup src/**/*.ts --dts",
    "test": "vitest --run"
  },
  "dependencies": {},
  "devDependencies": {
    "tsup": "^7.2.0",
    "typescript": "^5.3.3",
    "vitest": "^1.0.0"
  }
}

// =============================================================
// apps/chat/app/api/chat/route.ts â€” Next.js API (ëª¨ë¸ í˜¸ì¶œ)
// -------------------------------------------------------------
import { NextRequest } from 'next/server'
import path from 'node:path'
import { assemblePrompt } from '@ontomind/prompt/src/assemble'
import { callLLM } from '@ontomind/port/src/llm'

export const runtime = 'nodejs'

export async function POST(req: NextRequest) {
  const body = await req.json()
  const intentKey: string = body.intentKey
  const targetIds: string[] = body.targetIds || []
  const contextIds: string[] = body.contextIds || []
  const stream: boolean = !!body.stream

  try {
    const bundle = assemblePrompt(
      { intentKey, targetIds, contextIds, maxEvidence: 8 },
      {
        promptSummaryPath: path.join(process.cwd(), 'data', 'generated', 'prompt-summary.json'),
        intentsSchemaPath: path.join(process.cwd(), 'data', 'generated', 'intents.schema.json'),
        instancesDir: path.join(process.cwd(), 'data', 'instances')
      }
    )

    const result = await callLLM(bundle, { provider: 'openai', stream, intentKey })

    if (stream && result.stream) {
      // ìŠ¤íŠ¸ë¦¬ë°: ì›ë³¸ ë°”ë””ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜(í”„ë¡ íŠ¸ì—ì„œ ReadableStream ì²˜ë¦¬)
      return new Response(result.stream as any, {
        headers: { 'Content-Type': 'text/event-stream' }
      })
    }

    return new Response(JSON.stringify(result), { headers: { 'Content-Type': 'application/json' } })
  } catch (e: any) {
    return new Response(JSON.stringify({ ok: false, error: e?.message || String(e) }), { status: 400 })
  }
}

// =============================================================
// ë£¨íŠ¸ package.json ìŠ¤í¬ë¦½íŠ¸ ì¶”ê°€
// -------------------------------------------------------------
/*
{
  "scripts": {
    "oml:chat": "curl -s -X POST localhost:3000/api/chat -H 'Content-Type: application/json' -d '{\n  \"intentKey\": \"intent:diagnose@1.0.0\",\n  \"targetIds\": [\"project:Project/sample-1\"]\n}' | jq ."
  }
}
*/

// =============================================================
// ì‚¬ìš© ë°©ë²•
// -------------------------------------------------------------
/*
1) Step 2/4 ì™„ë£Œ ìƒíƒœì—ì„œ Next dev ì„œë²„ ì‹¤í–‰
2) POST /api/chat { intentKey, targetIds, contextIds?, stream? }
3) ë…¼ìŠ¤íŠ¸ë¦¬ë°ì´ë©´ JSON, ìŠ¤íŠ¸ë¦¬ë°ì´ë©´ text/event-streamìœ¼ë¡œ ìˆ˜ì‹ 
4) ì„œë²„ì—ì„œ ì‘ë‹µì„ íŒŒì‹± í›„ validateIntentOutputìœ¼ë¡œ ê²€ì¦(ë…¼ìŠ¤íŠ¸ë¦¬ë° ê²½ë¡œ)

ì£¼ì˜
- OpenAI í˜¸í™˜í˜• ëª¨ë¸ ì¤‘ response_format: json_schemaë¥¼ ì§€ì›í•˜ëŠ” ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”.
- ëª¨ë¸ì´ JSONì„ í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•  ë•Œë¥¼ ëŒ€ë¹„í•˜ì—¬ JSON.parse ì‹œë„ â†’ ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ì „ë‹¬ + ê²€ì¦ ì‹¤íŒ¨ ë¦¬í¬íŠ¸.
*/
